Below is a concise comparison table of the requested concepts and algorithms. Each row highlights the **paradigm**, **core idea**, a representative **strength**, and a notable **limitation** a graduate-level reader should keep in mind.

| **Technique**                            | **Paradigm / Category**        | **Core Idea (1-sentence)**                                                       | **Key Strength**                                           | **Key Limitation**                                                               |
| ---------------------------------------- | ------------------------------ | -------------------------------------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------------------------- |
| **AdaBoost**                             | Ensemble ¬∑ Boosting            | Sequentially re-weights hard-to-classify examples and combines weak learners.    | High accuracy with simple base models.                     | Sensitive to label noise and outliers.                                           |
| **Bagging**                              | Ensemble ¬∑ Bootstrap           | Trains models on bootstrap samples and averages/votes.                           | Strong variance reduction, combats over-fitting.           | Little bias reduction; increased computation.                                    |
| **Bayes Optimal Classifier**             | Probabilistic Theory           | Predicts by averaging over all hypotheses weighted by posterior probability.     | Theoretically minimal expected error.                      | Computationally infeasible except for trivial spaces.                            |
| **Bayesian Networks**                    | Probabilistic Graphical Model  | DAG encodes conditional independencies for joint distribution.                   | Interpretable causal reasoning; handles missing data.      | Structure/parameter learning is NP-hard; prior sensitive.                        |
| **Bellman Equation**                     | Reinforcement Learning ¬∑ DP    | Recursive relation linking a state‚Äôs value to successor values.                  | Fundamental for value-based RL algorithms.                 | Assumes known Markov model; scales poorly with state size.                       |
| **Boosting (meta-concept)**              | Ensemble Strategy              | Sequentially focuses on current errors to build a strong learner.                | Converts weak learners into powerful ones.                 | Prone to over-fit noisy data; sequential dependency.                             |
| **Ensemble Learning**                    | Meta-technique                 | Combines multiple diverse models to improve predictions.                         | Increases accuracy & robustness.                           | Adds complexity and lowers interpretability.                                     |
| **Expectation Maximization (EM)**        | Latent-variable Optimization   | Alternates E- & M-steps to maximize incomplete-data likelihood.                  | Handles missing data & mixture models elegantly.           | Only finds local maxima; may converge slowly.                                    |
| **Genetic Algorithms**                   | Evolutionary Search            | Populations evolve via selection, crossover, mutation.                           | Explores rugged, non-convex surfaces without gradients.    | Many fitness evaluations; parameter tuning brittle.                              |
| **ID3**                                  | Decision Trees                 | Greedy splits using information gain.                                            | Fast, easy to interpret.                                   | Over-fits noisy data; handles continuous features poorly without discretization. |
| **Independent Component Analysis (ICA)** | Unsupervised Signal Separation | Finds linear components that are statistically independent.                      | Recovers latent sources (e.g., EEG).                       | Requires non-Gaussian sources and linear mixing.                                 |
| **k-means**                              | Partitional Clustering         | Minimizes within-cluster SSE around k centroids.                                 | Scalable, simple to implement.                             | Requires $k$; assumes spherical clusters; sensitive to outliers.                 |
| **Na√Øve Bayes Classifier**               | Probabilistic ¬∑ Generative     | Applies Bayes‚Äô rule with conditional independence assumption.                    | Very fast; works well on high-dim sparse data.             | Independence rarely true ‚áí sub-optimal probability estimates.                    |
| **Perceptrons**                          | Linear Classifier (online)     | Updates weights when a sample is misclassified.                                  | Converges if data linearly separable; simple rule.         | Cannot model non-linear boundaries; no probability output.                       |
| **Policy Iteration**                     | RL ¬∑ Dynamic Programming       | Repeats policy evaluation then improvement until optimal.                        | Fewer iterations than value iteration; exact convergence.  | Requires full model; evaluation step costly for large MDPs.                      |
| **Principal Component Analysis (PCA)**   | Dimensionality Reduction       | Orthogonally projects data to maximize variance.                                 | Removes correlations, compresses noise.                    | Captures only linear structure; scale-sensitive.                                 |
| **Q-learning**                           | Model-free RL (off-policy)     | Learns action values via TD updates independent of behavior policy.              | Finds optimal policy without environment model.            | Instability with function approximation; exploration trade-off.                  |
| **Random Hill Climbing**                 | Local Search                   | Iteratively moves to a better neighboring state.                                 | Memory-light, easy to code.                                | Easily trapped in local optima.                                                  |
| **Random Projections**                   | Dimensionality Reduction       | Uses a random linear map that (JL lemma) preserves distances.                    | Extremely fast and data-independent.                       | Distorts some structure; not adaptive to data geometry.                          |
| **Regression Trees (CART)**              | Supervised, Non-linear         | Splits features to minimize squared error in leaves.                             | Captures complex interactions; interpretable.              | High variance; piecewise-constant predictions.                                   |
| **SARSA**                                | Model-free RL (on-policy)      | Updates Q based on the action actually taken (state-action-reward-state-action). | Safer alignment with exploration policy.                   | Slower convergence vs. off-policy methods.                                       |
| **Simulated Annealing**                  | Metaheuristic Global Search    | Accepts worse moves with decreasing probability (cooling schedule).              | Escapes local optima; simple implementation.               | Requires careful cooling schedule; can be slow.                                  |
| **Single Linkage Clustering**            | Hierarchical Agglomerative     | Merges clusters with minimum pairwise distance.                                  | Detects elongated/non-convex shapes.                       | ‚ÄúChaining‚Äù effect; noise-sensitive.                                              |
| **Soft Clustering**                      | Probabilistic Clustering       | Assigns fractional membership (e.g., GMM responsibilities).                      | Captures overlap & uncertainty.                            | Heavier computation; harder to interpret.                                        |
| **Support Vector Machines (SVM)**        | Margin-based Classifier        | Maximizes margin; kernel trick for non-linearities.                              | Convex optimization ‚áí global optimum; effective in high-d. | Slow for very large data; kernel/ùúÜ selection critical.                          |
| **Value Iteration**                      | RL ¬∑ Dynamic Programming       | Iteratively applies Bellman optimality backups to values.                        | Simple; guaranteed convergence.                            | Slow near convergence; needs transition model & full state sweep.                |
| **Weak Learners**                        | Theoretical Base Models        | Predict only slightly better than random guessing (‚â§ ¬Ω + Œµ).                     | Building blocks for boosting-style ensembles.              | Poor standalone accuracy; may have high bias.                                    |

_Tip:_ Use this table as a quick reference‚Äîeach entry signals **where** a method fits in the ML landscape, **why** it can shine, and **what caveats** temper its use.